{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'input.txt'\n",
    "#filename = 'wodehouse_right_ho_jeeves.txt'\n",
    "data_raw = open(filename, 'r').read() # should be simple plain text file\n",
    "data_raw = data_raw.lower()\n",
    "data_raw = data_raw[0:10009]\n",
    "\n",
    "table = str.maketrans(dict.fromkeys('ï»¿'))\n",
    "data_raw = data_raw.translate(table)\n",
    "\n",
    "data, vocab_size, idx_to_char, char_to_idx = data_from_text(data_raw)\n",
    "data_one_hot = make_one_hot(data, vocab_size)\n",
    "\n",
    "print( 'data_one_hot shape: ', data_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 25 # this is how much of the data we sample before updating the params\n",
    "hidden_size = 50 # size of the hidden state vector\n",
    "learning_rate = 0.1\n",
    "num_epochs = 50\n",
    "\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # create recurrent and linear layers\n",
    "        lstm = torch.nn.LSTM(input_size=self.vocab_size,hidden_size=self.hidden_size) \n",
    "        linear = torch.nn.Linear(self.hidden_size, self.vocab_size)\n",
    "\n",
    "        #self.rnn = rnn\n",
    "        self.lstm = lstm\n",
    "        self.linear = linear\n",
    "        self.reset_states()\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h, (h_prev, c_prev) = self.lstm(inputs, (self.h_prev, self.c_prev))\n",
    "        out = self.linear(h)\n",
    "        self.h_prev = h_prev.detach()\n",
    "        self.c_prev = c_prev.detach()\n",
    "        return out\n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.h_prev = torch.zeros([1, 1, self.hidden_size])\n",
    "        self.c_prev = torch.zeros([1, 1, self.hidden_size])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(vocab_size, hidden_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "#parameters = initialize_parameters(hidden_size, vocab_size)\n",
    "dtype = torch.float\n",
    "data_length = len(data)\n",
    "seqs_per_epoch = int(data_length/seq_length)  # will this work if data/seq divides exactly?\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "#optimizer = torch.optim.Adagrad(model.parameters(), lr=1e-1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "data3D = np.expand_dims(data_one_hot, 1)\n",
    "print( 'data3D shape: ', data3D.shape)\n",
    "for epoch in range(num_epochs):\n",
    "    # initialize hidden  state  \n",
    "    model.reset_states()\n",
    "\n",
    "    if verbose and epoch != 0:\n",
    "        print('epoch: {}, loss: {}'.format(epoch, loss))\n",
    "\n",
    "    for i in range(seqs_per_epoch):\n",
    "        start = i*seq_length\n",
    "        end = i*seq_length+seq_length\n",
    "        end = min(end, data_length-1)\n",
    "\n",
    "        inputs_raw = data3D[start:end]\n",
    "        targets_raw = data[start+1:end+1]\n",
    "             \n",
    "        # both inputs and targets are (seq_length, 1, vocab_size)\n",
    "        inputs = torch.tensor(inputs_raw, dtype=dtype)\n",
    "        targets = torch.tensor(targets_raw, dtype=torch.long)\n",
    "                \n",
    "        out = model(inputs)\n",
    "        \n",
    "        out2 = out.view((seq_length, vocab_size))\n",
    "        loss = loss_fn(out2, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         if i%10 == 0:\n",
    "#             print( i, loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_states()\n",
    "\n",
    "seed_string = 'my name is '\n",
    "sample = generate_sample(model, 5, [char_to_idx[i] for i in seed_string])\n",
    "''.join([idx_to_char[i] for i in sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
